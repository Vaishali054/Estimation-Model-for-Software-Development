{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "columns = ['Project', 'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Effort', 'Transactions', 'Entities', 'PointsAdjust', 'Envergure', 'PointsNonAjust', 'Language']\n",
    "df=pd.read_csv('./desharnais.txt', names=columns, comment='%', na_values='?', skipinitialspace=True, delimiter=',')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and data types\n",
    "df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic stats for dataset\n",
    "#mean\n",
    "# percentile for p=0.25 & 0.75\n",
    "# third quartile/median\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adequacy of data using heatmap\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class imbalance\n",
    "for col in columns[1:]:\n",
    "    df[col].plot(kind='hist', bins=20, title=f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for each numerical column\n",
    "for col in columns:\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean\n",
    "\n",
    "trim_frac=0.10\n",
    "for col in columns[1:]:\n",
    "    print(f'Trimmed Mean for {col} : {trim_mean(df[col], proportiontocut=trim_frac)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frac = 0.1\n",
    "\n",
    "trimmed_df = pd.DataFrame()\n",
    "\n",
    "for col in df.columns:\n",
    "    sorted_data = np.sort(df[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_column = np.full_like(df[col], np.nan)\n",
    "    trimmed_column[lower_idx:upper_idx] = trimmed_data\n",
    "    \n",
    "    trimmed_df[col] = trimmed_column\n",
    "\n",
    "print(trimmed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_median_dict = {}\n",
    "trimmed_std_dict = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    sorted_data = np.sort(df[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_median = np.median(trimmed_data)\n",
    "    trimmed_median_dict[col] = trimmed_median\n",
    "    \n",
    "    trimmed_std = np.std(trimmed_data, ddof=1) \n",
    "    trimmed_std_dict[col] = trimmed_std\n",
    "\n",
    "trimmed_median_df = pd.DataFrame(list(trimmed_median_dict.items()), columns=['Column', 'Trimmed Median'])\n",
    "trimmed_std_df = pd.DataFrame(list(trimmed_std_dict.items()), columns=['Column', 'Trimmed Std Deviation'])\n",
    "\n",
    "print(\"Trimmed Medians:\")\n",
    "print(trimmed_median_df[1:])\n",
    "\n",
    "print(\"\\nTrimmed Standard Deviations:\")\n",
    "print(trimmed_std_df[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCOMO81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "with open('./cocomo811.arff') as f:\n",
    "    dataset=arff.load(f)\n",
    "\n",
    "cocomo = pd.DataFrame(dataset['data'], columns=[attr[0] for attr in dataset['attributes']])\n",
    "\n",
    "cocomo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and data types\n",
    "cocomo.info()\n",
    "cocomo.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic stats for dataset\n",
    "#mean\n",
    "# percentile for p=0.25 & 0.75\n",
    "# third quartile/median\n",
    "cocomo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows\n",
    "cocomo.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adequacy of data using heatmap\n",
    "sns.heatmap(cocomo.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class imbalance\n",
    "for col in cocomo.columns:\n",
    "    cocomo[col].plot(kind='hist', bins=20, title=f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for each numerical column\n",
    "for col in cocomo.columns:\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.boxplot(y=cocomo[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frac=0.01\n",
    "for col in cocomo.columns:\n",
    "    print(f'Trimmed Mean for {col} : {trim_mean(cocomo[col], proportiontocut=trim_frac)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frac = 0.05\n",
    "\n",
    "trimmed_cocomo = pd.DataFrame()\n",
    "\n",
    "for col in cocomo.columns:\n",
    "    sorted_data = np.sort(cocomo[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_column_cocomo = np.full_like(cocomo[col], np.nan)\n",
    "    trimmed_column_cocomo[lower_idx:upper_idx] = trimmed_data\n",
    "    \n",
    "    trimmed_cocomo[col] = trimmed_column_cocomo\n",
    "\n",
    "trimmed_cocomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_cocomo_cleaned=trimmed_cocomo.dropna()\n",
    "\n",
    "trim_cocomo_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trimmed_median_dict = {}\n",
    "trimmed_std_dict = {}\n",
    "\n",
    "for col in trim_cocomo_cleaned.columns:\n",
    "    sorted_data = np.sort(trim_cocomo_cleaned[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_median = np.median(trimmed_data)\n",
    "    trimmed_median_dict[col] = trimmed_median\n",
    "    \n",
    "    trimmed_std = np.std(trimmed_data, ddof=1) \n",
    "    trimmed_std_dict[col] = trimmed_std\n",
    "\n",
    "trimmed_median_cocomo = pd.DataFrame(list(trimmed_median_dict.items()), columns=['Column', 'Trimmed Median'])\n",
    "trimmed_std_cocomo = pd.DataFrame(list(trimmed_std_dict.items()), columns=['Column', 'Trimmed Std Deviation'])\n",
    "\n",
    "trimmed_median_cocomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_std_cocomo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "with open('./china.arff') as f:\n",
    "    dataset=arff.load(f)\n",
    "\n",
    "china = pd.DataFrame(dataset['data'], columns=[attr[0] for attr in dataset['attributes']])\n",
    "\n",
    "china.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and data types\n",
    "china.info()\n",
    "china.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic stats for dataset\n",
    "#mean\n",
    "# percentile for p=0.25 & 0.75\n",
    "# third quartile/median\n",
    "china.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows\n",
    "china.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adequacy of data using heatmap\n",
    "# sns.heatmap(china.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(china.dtypes)\n",
    "print(china.head())\n",
    "\n",
    "#Encontured categorical data which is not in numeric form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting categorical data into numerical data\n",
    "print(china['DevType'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china['devtype_numeric'], devtype_mapping = pd.factorize(china['DevType'])\n",
    "print(devtype_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "china['devtype_numeric'] = le.fit_transform(china['DevType'])\n",
    "print(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = china.drop(columns=['DevType'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china.head()\n",
    "#Successfully converted categorical data into numerical where 0='NewDev' & 1= 'Maint'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adequacy of data using heatmap\n",
    "sns.heatmap(china.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking for class imbalance\n",
    "# for col in china.columns:\n",
    "#     china[col].plot(kind='hist', bins=20, title=f'Distribution of {col}')\n",
    "#     plt.xlabel(col)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem : Although Resource appears in numerical form it is of type object {1,2,3,4}, i.e. chnaging it to numeric\n",
    "\n",
    "china[china.columns]=china[china.columns].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class imbalance\n",
    "for col in china.columns:\n",
    "    china[col].plot(kind='hist', bins=20, title=f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for each numerical column\n",
    "for col in china.columns:\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.boxplot(y=china[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frac=0.1\n",
    "for col in china.columns:\n",
    "    print(f'Trimmed Mean for {col} : {trim_mean(china[col], proportiontocut=trim_frac)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_china = pd.DataFrame()\n",
    "trim_frac=0.01\n",
    "for col in china.columns:\n",
    "    sorted_data = np.sort(china[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_column_china = np.full_like(china[col], np.nan)\n",
    "    trimmed_column_china[lower_idx:upper_idx] = trimmed_data\n",
    "    \n",
    "    trimmed_china[col] = trimmed_column_china\n",
    "\n",
    "trimmed_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_china_cleaned=trimmed_china.dropna()\n",
    "\n",
    "trim_china_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_median_dict = {}\n",
    "trimmed_std_dict = {}\n",
    "\n",
    "for col in trim_china_cleaned.columns:\n",
    "    sorted_data = np.sort(trim_china_cleaned[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_median = np.median(trimmed_data)\n",
    "    trimmed_median_dict[col] = trimmed_median\n",
    "    \n",
    "    trimmed_std = np.std(trimmed_data, ddof=1) \n",
    "    trimmed_std_dict[col] = trimmed_std\n",
    "\n",
    "trimmed_median_china = pd.DataFrame(list(trimmed_median_dict.items()), columns=['Column', 'Trimmed Median'])\n",
    "trimmed_std_china = pd.DataFrame(list(trimmed_std_dict.items()), columns=['Column', 'Trimmed Std Deviation'])\n",
    "\n",
    "trimmed_median_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_std_china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kitchenham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "with open('./kitchenham.arff') as f:\n",
    "    dataset=arff.load(f)\n",
    "\n",
    "kitchenham = pd.DataFrame(dataset['data'], columns=[attr[0] for attr in dataset['attributes']])\n",
    "\n",
    "kitchenham.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and data types\n",
    "kitchenham.info()\n",
    "kitchenham.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic stats for dataset\n",
    "#mean\n",
    "# percentile for p=0.25 & 0.75\n",
    "# third quartile/median\n",
    "kitchenham.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows\n",
    "kitchenham.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adequacy of data using heatmap\n",
    "sns.heatmap(kitchenham.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class imbalance\n",
    "for col in kitchenham.columns:\n",
    "    kitchenham[col].plot(kind='hist', bins=20, title=f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for each numerical column\n",
    "for col in kitchenham.columns:\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.boxplot(y=kitchenham[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frac=0.05\n",
    "for col in kitchenham.columns:\n",
    "    print(f'Trimmed Mean for {col} : {trim_mean(kitchenham[col], proportiontocut=trim_frac)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_kitchenham = pd.DataFrame()\n",
    "\n",
    "for col in kitchenham.columns:\n",
    "    sorted_data = np.sort(kitchenham[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_column_kitchenham = np.full_like(kitchenham[col], np.nan)\n",
    "    trimmed_column_kitchenham[lower_idx:upper_idx] = trimmed_data\n",
    "    \n",
    "    trimmed_kitchenham[col] = trimmed_column_kitchenham\n",
    "\n",
    "trimmed_kitchenham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_kitchenham_cleaned=trimmed_kitchenham.dropna()\n",
    "\n",
    "trim_kitchenham_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_median_dict = {}\n",
    "trimmed_std_dict = {}\n",
    "\n",
    "for col in trim_kitchenham_cleaned.columns:\n",
    "    sorted_data = np.sort(trim_kitchenham_cleaned[col])\n",
    "    \n",
    "    lower_idx = int(len(sorted_data) * trim_frac)\n",
    "    upper_idx = int(len(sorted_data) * (1 - trim_frac))\n",
    "    \n",
    "    trimmed_data = sorted_data[lower_idx:upper_idx]\n",
    "    \n",
    "    trimmed_median = np.median(trimmed_data)\n",
    "    trimmed_median_dict[col] = trimmed_median\n",
    "    \n",
    "    trimmed_std = np.std(trimmed_data, ddof=1) \n",
    "    trimmed_std_dict[col] = trimmed_std\n",
    "\n",
    "trimmed_median_kitchenham = pd.DataFrame(list(trimmed_median_dict.items()), columns=['Column', 'Trimmed Median'])\n",
    "trimmed_std_kitchenham = pd.DataFrame(list(trimmed_std_dict.items()), columns=['Column', 'Trimmed Std Deviation'])\n",
    "\n",
    "trimmed_median_kitchenham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_std_kitchenham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "df=pd.read_csv('./desharnais.txt', names=columns, comment='%', skipinitialspace=True, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.isna().sum()\n",
    "\n",
    "print(df['TeamExp'].unique())\n",
    "print(df['ManagerExp'].unique())\n",
    "\n",
    "df['TeamExp'] = df['TeamExp'].replace('?', np.nan)\n",
    "df['TeamExp'] = pd.to_numeric(df['TeamExp'], errors='coerce')\n",
    "\n",
    "# Step 3: Replace NaN with the median (or mean if you prefer)\n",
    "df['TeamExp'].fillna(df['TeamExp'].median(), inplace=True)\n",
    "df['ManagerExp'] = df['ManagerExp'].replace('?', np.nan)\n",
    "df['ManagerExp'] = pd.to_numeric(df['ManagerExp'], errors='coerce')\n",
    "\n",
    "# Step 3: Replace NaN with the median (or mean if you prefer)\n",
    "df['ManagerExp'].fillna(df['ManagerExp'].median(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in 'TeamExp' with the median value\n",
    "df['TeamExp'].fillna(df['TeamExp'].median(), inplace=True)\n",
    "\n",
    "# Replace NaN in 'ManagerExp' with the median value\n",
    "df['ManagerExp'].fillna(df['ManagerExp'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['TeamExp'].unique())\n",
    "print(df['ManagerExp'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Effort'])  # 'Effort' is assumed to be the target variable\n",
    "y = df['Effort']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming X is your feature set and y is your target variable\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive and calculate RMSE\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(f'Cross-Validation RMSE Scores: {rmse_scores}')\n",
    "print(f'Mean RMSE: {rmse_scores.mean()}')\n",
    "print(f'Standard Deviation of RMSE: {rmse_scores.std()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "data=pd.read_csv('./desharnais.txt', names=columns, comment='%', skipinitialspace=True, delimiter=',')\n",
    "\n",
    "# Preprocessing: Replace '?' with NaN, and handle NaNs (for experience-related columns)\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data['TeamExp'] = pd.to_numeric(data['TeamExp'], errors='coerce').fillna(0)\n",
    "data['ManagerExp'] = pd.to_numeric(data['ManagerExp'], errors='coerce').fillna(0)\n",
    "\n",
    "# Features and target variable\n",
    "X = data[['TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities', 'PointsAdjust', 'Envergure', 'PointsNonAjust', 'Language']]\n",
    "y = data['Effort']\n",
    "\n",
    "# Split the dataset into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data without cross-validation\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance without cross-validation\n",
    "mse_before = mean_squared_error(y_test, y_pred)\n",
    "mae_before = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# 10-Fold Cross-Validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "rmse_cv_scores = np.sqrt(-cv_scores)\n",
    "\n",
    "# Average RMSE from cross-validation\n",
    "mean_rmse_cv = rmse_cv_scores.mean()\n",
    "std_rmse_cv = rmse_cv_scores.std()\n",
    "\n",
    "# Print results\n",
    "print(\"Performance Without Cross-Validation:\")\n",
    "print(f\"Mean Squared Error: {mse_before:.2f}\")\n",
    "print(f\"Mean Absolute Error: {mae_before:.2f}\")\n",
    "\n",
    "print(\"\\nPerformance With 10-Fold Cross-Validation:\")\n",
    "print(f\"Mean RMSE: {mean_rmse_cv:.2f}\")\n",
    "print(f\"Standard Deviation of RMSE: {std_rmse_cv:.2f}\")\n",
    "\n",
    "# Optional: Visualize the cross-validation results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), rmse_cv_scores, marker='o', linestyle='-', color='blue')\n",
    "plt.title('RMSE for Each Fold in 10-Fold Cross-Validation')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['TeamExp', 'ManagerExp', 'Length', 'Transactions', 'Entities', 'PointsAdjust', 'Envergure', 'Language']]\n",
    "y = data['Effort']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 for the initial model\n",
    "r2_before = r2_score(y_test, y_pred)\n",
    "accuracy_before = r2_before * 100  # Convert to percentage\n",
    "\n",
    "# Print before K-fold R^2\n",
    "print(f'R-squared before K-fold: {accuracy_before:.2f}%')\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "k_fold_r2_scores = cross_val_score(model, X, y, cv=10, scoring='r2')\n",
    "accuracy_after = np.mean(k_fold_r2_scores) * 100  # Convert to percentage\n",
    "\n",
    "# Print after K-fold R^2\n",
    "print(f'Average R-squared after K-fold: {accuracy_after:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "# Create and fit the SVR model\n",
    "svr_model = SVR(kernel='linear')  # You can change the kernel to 'rbf', 'poly', etc.\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 for the initial model\n",
    "r2_before = r2_score(y_test, y_pred)\n",
    "accuracy_before = r2_before * 100  # Convert to percentage\n",
    "\n",
    "# Print before K-fold R^2\n",
    "print(f'R-squared before K-fold (SVR): {accuracy_before:.2f}%')\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "k_fold_r2_scores = cross_val_score(svr_model, X, y, cv=10, scoring='r2')\n",
    "accuracy_after = np.mean(k_fold_r2_scores) * 100  # Convert to percentage\n",
    "\n",
    "# Print after K-fold R^2\n",
    "print(f'Average R-squared after K-fold (SVR): {accuracy_after:.2f}%')\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root Mean Squared Error (SVR): {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create and fit the Decision Tree model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 for the initial model\n",
    "r2_dt_before = r2_score(y_test, y_pred_dt)\n",
    "accuracy_dt_before = r2_dt_before * 100  # Convert to percentage\n",
    "\n",
    "# Print before K-fold R^2 for Decision Tree\n",
    "print(f'R-squared before K-fold (Decision Tree): {accuracy_dt_before:.2f}%')\n",
    "\n",
    "# Perform K-fold cross-validation for Decision Tree\n",
    "k_fold_r2_dt_scores = cross_val_score(dt_model, X, y, cv=10, scoring='r2')\n",
    "accuracy_dt_after = np.mean(k_fold_r2_dt_scores) * 100  # Convert to percentage\n",
    "\n",
    "# Print after K-fold R^2 for Decision Tree\n",
    "print(f'Average R-squared after K-fold (Decision Tree): {accuracy_dt_after:.2f}%')\n",
    "\n",
    "# Calculate and print RMSE for Decision Tree\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(f'Root Mean Squared Error (Decision Tree): {rmse_dt:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv') # Replace with your dataset loading method\n",
    "\n",
    "# Define your features and target variable\n",
    "X = df[['TeamExp', 'ManagerExp', 'Length', 'Transactions', 'Entities', 'PointsAdjust', 'Envergure', 'Language']]\n",
    "y = df['Effort']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the Decision Tree model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate R^2 for the initial model\n",
    "r2_dt_before = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "# Print before K-fold R^2 for Decision Tree\n",
    "print(f'R-squared before K-fold (Decision Tree): {r2_dt_before:.4f}')\n",
    "\n",
    "# Perform K-fold cross-validation for Decision Tree\n",
    "k_fold_r2_dt_scores = cross_val_score(dt_model, X, y, cv=10, scoring='r2')\n",
    "average_r2_dt_after = np.mean(k_fold_r2_dt_scores)\n",
    "\n",
    "# Print after K-fold R^2 for Decision Tree\n",
    "print(f'Average R-squared after K-fold (Decision Tree): {average_r2_dt_after:.4f}')\n",
    "\n",
    "# Calculate and print RMSE for Decision Tree\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(f'Root Mean Squared Error (Decision Tree): {rmse_dt:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset\n",
    "data=pd.read_csv('./desharnais.txt', names=columns, comment='%', skipinitialspace=True, delimiter=',')\n",
    "\n",
    "# Train-test split\n",
    "X = data[['TeamExp', 'ManagerExp', 'Length', 'Transactions', 'Entities', 'PointsAdjust', 'Envergure', 'Language']]\n",
    "y = data['Effort']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models, including Random Forest, Gradient Boosting, and KNN\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Support Vector Regression': SVR(kernel='linear'),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'KNN': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Train models and evaluate performance\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Perform 10-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    mean_cv_score = -np.mean(cv_scores)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'Mean CV MSE': mean_cv_score,\n",
    "        'RMSE': rmse,\n",
    "        'R^2': r2\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Mean CV MSE: {metrics['Mean CV MSE']:.2f}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.2f}\")\n",
    "    print(f\"  R^2: {metrics['R^2']:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
